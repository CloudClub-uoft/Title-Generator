{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'torch'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info.\n",
      "\u001b[1;31mSome of the following files found in the working directory may have prevented the Kernel from starting. Consider renaming them.\n",
      "\u001b[1;31mFile(s): <a href='file:///c%3A/Users/qingy/OneDrive/Documents/CloudClub/Title-Generator/tokenize.py?line=1'>~\\tokenize.py</a> might need to be renamed.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/JupyterKernelStartFailureOverrideReservedName'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary class\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold: int):\n",
    "        '''any word that appears below freq_threshold number of times will not be included in the vocabulary'''\n",
    "        self.freq_threshold = freq_threshold\n",
    "        # index to string\n",
    "        self.itos = { 0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\" }\n",
    "        # string to index\n",
    "        self.stoi = { \"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3 }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer(text: str) -> list[str]:\n",
    "        # TODO: tokenize text as done in preprocessing\n",
    "        return text.split()\n",
    "\n",
    "    def build_vocabulary(self, sentence_list: list[str]):\n",
    "        frequencies = {}\n",
    "        i = len(self.itos) # currently 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            print(sentence)\n",
    "            for word in self.tokenizer(sentence):\n",
    "        \n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                if frequencies[word] >= self.freq_threshold:\n",
    "                    self.stoi[word] = i\n",
    "                    self.itos[i] = word\n",
    "                    i += 1\n",
    "\n",
    "    def numericalize(self, text: str) -> list[int]:\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello this is a sentece\n",
      "this is another sentence\n",
      "{0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>', 4: 'hello', 5: 'this', 6: 'is', 7: 'a', 8: 'sentece', 9: 'this', 10: 'is', 11: 'another', 12: 'sentence'}\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'hello': 4, 'this': 9, 'is': 10, 'a': 7, 'sentece': 8, 'another': 11, 'sentence': 12}\n",
      "[9, 10, 12]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(0)\n",
    "sentences = [\"hello this is a sentece\", \"this is another sentence\"]\n",
    "vocab.build_vocabulary(sentences)\n",
    "\n",
    "print(vocab.itos)\n",
    "print(vocab.stoi)\n",
    "print(vocab.numericalize(\"this is sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path: str, freq_threshold=5):\n",
    "        self.dir = dir\n",
    "        self.df = pd.read_csv(file_path)\n",
    "\n",
    "        self.paragraphs = self.df[\"paragraph\"]\n",
    "        self.titles = self.df[\"title\"]\n",
    "\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.paragraphs.tolist() + self.titles.tolist())\n",
    "        \n",
    "        self.word_to_paragraph = {}\n",
    "\n",
    "        paragraph_index = 0 # paragraph that this word in title belongs to\n",
    "        word_rank = 0 # position (index) in the current title\n",
    "        for word_index, word in enumerate(self.titles):\n",
    "            self.word_to_paragraph[word_index] = (paragraph_index, word_rank)\n",
    "            if word == \"<EOS>\":\n",
    "                paragraph_index += 1\n",
    "                word_rank = 0\n",
    "            else:\n",
    "                word_rank += 1\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        paragraph_index = self.word_to_paragraph[index]\n",
    "        paragraph, cur_rank = self.paragraphs[paragraph_index]\n",
    "        next_word = self.titles[index]\n",
    "        title_so_far = self.titles[index-cur_rank:index]\n",
    "\n",
    "        # TODO: convert title to one-hot encoded before returning\n",
    "\n",
    "        return (paragraph, title_so_far), next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(\n",
    "    file_path: str,\n",
    "    dataset: MyDataset,\n",
    "    freq_threshold=5,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    splits=[0.8,0.1,0.1]):\n",
    "    '''\n",
    "    dataset: torchvision.datasets a transformer dataset for training, testing, and validation\n",
    "    batch_size: int\n",
    "    splits: list(str) train-validation-test split\n",
    "    return: DataLoader\n",
    "    '''\n",
    "    dataset = TrainDataset(file_path, freq_threshold)\n",
    "\n",
    "    assert sum(splits) == 1, \"ensure sum of train-validation-test split adds up to 1\"\n",
    "\n",
    "    # perform split\n",
    "    size = len(dataset)\n",
    "    l1, l2 = int(size*splits[0]), int(size*splits[1])\n",
    "    l3 = size - l1 - l2\n",
    "\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "        dataset,\n",
    "        [l1, l2, l3],\n",
    "        generator=torch.Generator().manual_seed(999)\n",
    "    )\n",
    "\n",
    "    # get data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader: DataLoader, model, loss_function, optimizer, scheduler=None, epochs=30): \n",
    "    losses_over_epochs = []\n",
    "    num_batches = len(data_loader)\n",
    "\n",
    "    for epoch in epochs:\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "        for (paragraphs, titles) in data_loader:\n",
    "            # forward step\n",
    "            out = model(paragraphs)\n",
    "\n",
    "            # loss\n",
    "            loss = loss_function(out, titles)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # back propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # learning rate scheduler update\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # finished one epoch of training\n",
    "        end = time.time()\n",
    "        print(f\"Completed epoch {epoch+1} | average loss: {total_loss/num_batches} | time: {end-start}s\")\n",
    "        losses_over_epochs.append(total_loss/num_batches)\n",
    "\n",
    "    return losses_over_epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a099891ed62022502e10f6b3054a07274276af51d501e11e31fecf1f0f9d485"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
