{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader # our custom Dataset class inherits Dataset\n",
    "import torch.utils.data\n",
    "import pandas as pd # to read .csv data\n",
    "import re # for tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence # for MyCollate class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary class\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold: int):\n",
    "        '''any word that appears below freq_threshold number of times will not be included in the vocabulary'''\n",
    "        self.freq_threshold = freq_threshold\n",
    "        # index to string\n",
    "        self.itos = { 0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\" }\n",
    "        # string to index\n",
    "        self.stoi = { \"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3 }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer(text: str) -> list[str]:\n",
    "        return re.findall(r\"[\\w']+|[.,!?:;]\", text.lower())\n",
    "\n",
    "    def build_vocabulary(self, sentence_list: list[str]):\n",
    "        frequencies = {}\n",
    "        i = len(self.itos) # currently 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = i\n",
    "                    self.itos[i] = word\n",
    "                    i += 1\n",
    "\n",
    "    def numericalize(self, text: str) -> list[int]:\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello this is a sentece\n",
      "this is another sentence\n",
      "{0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>', 4: 'hello', 5: 'this', 6: 'is', 7: 'a', 8: 'sentece', 9: 'this', 10: 'is', 11: 'another', 12: 'sentence'}\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'hello': 4, 'this': 9, 'is': 10, 'a': 7, 'sentece': 8, 'another': 11, 'sentence': 12}\n",
      "[9, 10, 12]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"test.csv\")\n",
    "paragraphs = df[\"paragraph\"]\n",
    "titles = df[\"title\"]\n",
    "print(paragraphs[0])\n",
    "print(titles.to_list())\n",
    "\n",
    "vocab = Vocabulary(1)\n",
    "print(vocab.tokenizer(\"HELLO this is a sentence.\"))\n",
    "\n",
    "vocab.build_vocabulary(paragraphs + titles)\n",
    "\n",
    "print(vocab.itos)\n",
    "print(vocab.stoi)\n",
    "print(vocab.numericalize(\"this is a sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, file_path: str, freq_threshold=5):\n",
    "        self.dir = dir\n",
    "        self.df = pd.read_csv(file_path)\n",
    "\n",
    "        self.paragraphs = self.df[\"paragraph\"]\n",
    "        self.titles = self.df[\"title\"]\n",
    "\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.paragraphs.tolist() + self.titles.tolist())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        paragraph = self.paragraphs[index]\n",
    "        title = self.titles[index]\n",
    "\n",
    "        # TODO: convert title to one-hot encoded before returning\n",
    "\n",
    "        numerialized_paragraph = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numerialized_paragraph += self.vocab.numericalize(paragraph)\n",
    "        numerialized_paragraph.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        numerialized_title = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numerialized_title += self.vocab.numericalize(title)\n",
    "        numerialized_title.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return torch.tensor(numerialized_paragraph), torch.tensor(numerialized_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_index):\n",
    "        self.pad_index = pad_index\n",
    "    \n",
    "    # __call__ is a default method that runs whenever obj(batch) is called\n",
    "    def __call__(self, batch):\n",
    "        # get all source indexed sentences of the batch\n",
    "        paragraphs = [item[0] for item in batch] \n",
    "        # pad them using pad_sequence method from pytorch. \n",
    "        paragraphs = pad_sequence(paragraphs, batch_first=False, padding_value = self.pad_index) \n",
    "        \n",
    "        # get all target indexed sentences of the batch\n",
    "        titles = [item[1] for item in batch] \n",
    "        # pad them using pad_sequence method from pytorch. \n",
    "        titles = pad_sequence(titles, batch_first=False, padding_value = self.pad_index)\n",
    "        return paragraphs, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(\n",
    "    file_path: str,\n",
    "    freq_threshold=5,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    splits=[0.8,0.1,0.1]):\n",
    "    '''\n",
    "    dataset: torchvision.datasets a transformer dataset for training, testing, and validation\n",
    "    batch_size: int\n",
    "    splits: list(str) train-validation-test split\n",
    "    return: DataLoader\n",
    "    '''\n",
    "    dataset = TrainDataset(file_path, freq_threshold)\n",
    "    pad_index = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    assert sum(splits) == 1, \"ensure sum of train-validation-test split adds up to 1\"\n",
    "\n",
    "    # only training set\n",
    "    if splits == [1,0,0]:\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=MyCollate(pad_index))\n",
    "        return train_loader, None, None\n",
    "        \n",
    "    # perform split\n",
    "    size = len(dataset)\n",
    "    l1, l2 = int(size*splits[0]), int(size*splits[1])\n",
    "    l3 = size - l1 - l2\n",
    "\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "        dataset,\n",
    "        [l1, l2, l3],\n",
    "        generator=torch.Generator().manual_seed(999)\n",
    "    )\n",
    "\n",
    "    # get data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"test.csv\"\n",
    "TestDataset = TrainDataset(path, 1)\n",
    "train_loader, val_loader, test_loader = get_data_loader(path, 1, 2, 2, [1,0,0])\n",
    "# recall special indices: { 0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\" }\n",
    "for i, (paragraphs, titles) in enumerate(train_loader):\n",
    "    print(paragraphs.shape)\n",
    "    print(titles.shape)\n",
    "    print(paragraphs)\n",
    "    print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader: DataLoader, model, loss_function, optimizer, scheduler=None, epochs=30): \n",
    "    losses_over_epochs = []\n",
    "    num_batches = len(data_loader)\n",
    "\n",
    "    for epoch in epochs:\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "        for (paragraphs, titles) in data_loader:\n",
    "            # forward step\n",
    "            out = model(paragraphs)\n",
    "\n",
    "            # loss\n",
    "            loss = loss_function(out, titles)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # back propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # learning rate scheduler update\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # finished one epoch of training\n",
    "        end = time.time()\n",
    "        print(f\"Completed epoch {epoch+1} | average loss: {total_loss/num_batches} | time: {end-start}s\")\n",
    "        losses_over_epochs.append(total_loss/num_batches)\n",
    "\n",
    "    return losses_over_epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a2bd2daeb0c020ce43de5ad165562b29120817cbbcc4b7133ff9fcae7210d52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
