{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, input_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_heads = embed_dim // num_heads     # dim_heads aka d_k\n",
    "\n",
    "        self.q_lin = nn.Linear(input_dim, embed_dim)\n",
    "        self.k_lin = nn.Linear(input_dim, embed_dim)\n",
    "        self.v_lin = nn.Linear(input_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # NOTE: split q, k, v\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        num_heads, dim_heads = self.num_heads, self.dim_heads\n",
    "\n",
    "        q = self.q_lin(q).reshape(batch_size, -1, num_heads, dim_heads).transpose(1, 2)\n",
    "        k = self.k_lin(k).reshape(batch_size, -1, num_heads, dim_heads).transpose(1, 2)\n",
    "        v = self.v_lin(v).reshape(batch_size, -1, num_heads, dim_heads).transpose(1, 2)\n",
    "\n",
    "        scores = attention(q, k, v, dim_heads, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "        scores = scores.transpose(1, 2).contiguous().reshape(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        output = self.out_proj(scores)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# this is qingyuan's function\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    scaled_dot = torch.matmul(q, k.transpose(-2, -1)) / sqrt(d_k)\n",
    "    scaled_dot = F.softmax(scaled_dot, dim=-1)\n",
    "    if dropout is not None:\n",
    "        scaled_dot = dropout(scaled_dot)\n",
    "    output =  torch.matmul(scaled_dot, v)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/natalievolk/Desktop/Volunteering and Extracurriculars/Cloud Club/Title-Generator/multi_head_attention.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000002?line=6'>7</a>\u001b[0m input_dim \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000002?line=8'>9</a>\u001b[0m mh \u001b[39m=\u001b[39m MultiHeadAttention(num_heads, embed_dim, input_dim)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000002?line=9'>10</a>\u001b[0m scores \u001b[39m=\u001b[39m mh\u001b[39m.\u001b[39;49mforward(q, k, v)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000002?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(scores)\n",
      "\u001b[1;32m/Users/natalievolk/Desktop/Volunteering and Extracurriculars/Cloud Club/Title-Generator/multi_head_attention.ipynb Cell 2'\u001b[0m in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000001?line=21'>22</a>\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_lin(k)\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, num_heads, dim_heads)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000001?line=22'>23</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_lin(v)\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, num_heads, dim_heads)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000001?line=24'>25</a>\u001b[0m scores \u001b[39m=\u001b[39m attention(q, k, v, dim_heads, mask\u001b[39m=\u001b[39;49mmask, dropout\u001b[39m=\u001b[39;49mdropout)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000001?line=26'>27</a>\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000001?line=28'>29</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_proj(scores)\n",
      "\u001b[1;32m/Users/natalievolk/Desktop/Volunteering and Extracurriculars/Cloud Club/Title-Generator/multi_head_attention.ipynb Cell 2'\u001b[0m in \u001b[0;36mattention\u001b[0;34m(q, k, v, d_k, mask, dropout)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000001?line=36'>37</a>\u001b[0m scaled_dot \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(scaled_dot, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000001?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m dropout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000001?line=38'>39</a>\u001b[0m     scaled_dot \u001b[39m=\u001b[39m dropout(scaled_dot)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000001?line=39'>40</a>\u001b[0m output \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39mmatmul(scaled_dot, v)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/natalievolk/Desktop/Volunteering%20and%20Extracurriculars/Cloud%20Club/Title-Generator/multi_head_attention.ipynb#ch0000001?line=40'>41</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "q = torch.tensor([[0, 10, 0]], dtype=torch.float32)\n",
    "k = torch.tensor([[0, 10, 0]], dtype=torch.float32)\n",
    "v = torch.tensor([[0, 10, 0]], dtype=torch.float32)\n",
    "input_dim = 3\n",
    "\n",
    "mh = MultiHeadAttention(num_heads, embed_dim, input_dim)\n",
    "scores = mh.forward(q, k, v)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e28aa8f00749c703ae5b639866da39d3cef40d6068bc8b931f821ff6e0150f69"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cloudai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
