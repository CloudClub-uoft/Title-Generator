{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, input_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.qkv_lin = nn.Linear(input_dim, embed_dim*3)    # bias=False?\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "    def forward(self, qkv, mask=None, dropout=None):\n",
    "        batch_size, seq_length, embed_dim = qkv.size()\n",
    "        num_heads, head_dim = self.num_heads, self.head_dim\n",
    "\n",
    "        qkv = self.qkv_lin(qkv)\n",
    "        qkv = qkv.reshape(batch_size, seq_length, num_heads, head_dim).permute(0, 2, 1, 3)\n",
    "        q,k,v = qkv.chunk(3, -1)\n",
    "\n",
    "        # NOTE: no mask parameter??\n",
    "        scores = attention(q, k, v, head_dim, dropout) #mask ?\n",
    "        \n",
    "        scores = scores.permute(0, 2, 1, 3).reshape(batch_size, seq_length, embed_dim)\n",
    "\n",
    "        output = self.out_proj(scores)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# DUMMY FUNCTION --> this is qingyuan's function\n",
    "def attention(q, k, v, mask=None):\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
